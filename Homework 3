from sklearn import linear_model
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression

#Create linear model
Boston=load_boston()
boston = pd.DataFrame(Boston.data)
print(Boston.feature_names)
boston.columns = Boston.feature_names
boston['PRICE'] = Boston.target
X = boston.drop('PRICE', axis = 1)
Y = boston['PRICE']
lm=LinearRegression()
model=lm.fit(X,Y)
coef_df = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
coef_df

##NOX (Nitrous Oxide Concentration) has the biggest overall influence on the price at -17.7


import matplotlib.pyplot as plt
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_wine

wine=load_wine()
data=wine.data
wine.feature_names

X = data[:, :2]
Y=wine.target


#Create KMeans Models
km = KMeans(n_clusters = 3, n_jobs = 4, random_state=21)
km.fit(X)

km3 = KMeans(n_clusters = 3, n_jobs = 4, random_state=21)
km3.fit(X)

km5 = KMeans(n_clusters = 5, n_jobs = 4, random_state=21)
km5.fit(X)

centers = km.cluster_centers_
print(centers)

#Create Plots

new_labels3 = km3.labels_
new_labels5 = km5.labels_

fig, axes = plt.subplots(1, 2, figsize=(16,8))
axes[0].scatter(X[:, 0], X[:, 1], c=new_labels3, cmap='gist_rainbow',
edgecolor='k', s=150)
axes[1].scatter(X[:, 0], X[:, 1], c=new_labels5, cmap='jet',
edgecolor='k', s=150)
axes[0].set_xlabel('alcohol', fontsize=18)
axes[0].set_ylabel('total_phenols', fontsize=18)
axes[1].set_xlabel('alcohol', fontsize=18)
axes[1].set_ylabel('total_phenols', fontsize=18)
axes[0].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)
axes[1].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)
axes[0].set_title('3 Clusters', fontsize=18)
axes[1].set_title('5 Clusters', fontsize=18)
axes[0].scatter(km3.cluster_centers_[:, 0], km3.cluster_centers_[:, 1], s=300, c='red')
axes[1].scatter(km5.cluster_centers_[:, 0], km5.cluster_centers_[:, 1], s=300, c='red')

#Elbow Method
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Four clusters appears to be the optimal amount of clusters
